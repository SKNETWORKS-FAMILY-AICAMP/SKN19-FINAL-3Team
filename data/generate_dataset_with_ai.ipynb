{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42131e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from modules.data_parsing import mask_links\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "category_list = [[],[],[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03cba688",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"file_004\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08126c5",
   "metadata": {},
   "source": [
    "## 기초 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8615f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "당신은 JSONL 학습 데이터 생성 전문가입니다.\n",
    "역할:\n",
    "- 입력 문서를 의미 단위로 섹션화\n",
    "- 각 섹션마다 JSON 객체 생성\n",
    "\n",
    "규칙:\n",
    "1. 출력 JSON 객체의 키:\n",
    "   - \"text\": 원본 섹션 내용 그대로\n",
    "   - \"similar_text\": 형식과 문장이 유사하지만 약간 다르게 표현된 내용\n",
    "   - \"warning_text\": 형식은 유사하지만 문맥이 다른 내용\n",
    "   - \"summary\": 섹션 한 줄 요약\n",
    "2. 섹션 사이 개행, 들여쓰기, 리스트 등 포맷 유지\n",
    "3. 섹션들을 결합하면 원본 문서와 100% 동일\n",
    "4. 출력은 JSONL 형식 (한 줄 = 한 섹션)\n",
    "5. 모든 문서는 GPT-5.1 모델을 기준으로 처리\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3bdf1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_jsonl(doc):\n",
    "    user_prompt = f\"\"\"\n",
    "        다음 문서를 JSONL 학습용 데이터로 변환해주세요. \n",
    "        문서 내용은 아래와 같습니다:\n",
    "\n",
    "        {doc}\n",
    "\n",
    "        pgsql\n",
    "        코드 복사\n",
    "\n",
    "        출력 규칙은 시스템 프롬프트를 참고하여 그대로 적용합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-5.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    \n",
    "    # 모델 출력 가져오기\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33a9c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "md_path = Path(f\"docs_data/raw_file/{FILE_NAME}.md\")\n",
    "\n",
    "with md_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    doc = f.read()\n",
    "\n",
    "url_registry = {}\n",
    "\n",
    "masking_path = f\"docs_data/masking_data/masking_{FILE_NAME}.json\"\n",
    "masked_text, link_map, next_counter = mask_links(doc, url_registry, start_idx=1, save_json_path=masking_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a48e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_text = convert_to_jsonl(masked_text)\n",
    "\n",
    "# 9. JSONL 줄 단위로 나누기\n",
    "jsonl_lines = jsonl_text.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "231d9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"docs_data/test_data/{FILE_NAME}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in jsonl_lines:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c6c05",
   "metadata": {},
   "source": [
    "## 임베딩 학습 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4045abfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['스팀오리 프로그램 원리', 'parent-document-retrieval', 'file_001', 'file_003', 'nGrinder 트래픽테스트', '예비군 동원훈련', 'chunking-strategy', '간단 IT 용어', '네이버 캠프 정보', 'file_005', '어텐션 설명', 'file_101', 'openai Whisper 사용법', 'file_004', 'file_201', 'file_102', 'DPO 설명', 'SKN 캠프 정보', 'FFNN 설명', 'AI 개발환경 정보', '소켓통신 + 마스킹 훈련법', 'LSTM 변형모델', 'Qwen MoE 구조', '애자일 방법론 설명', 'prompt-flow', 'bedrock-agent', 'query-transformation']\n"
     ]
    }
   ],
   "source": [
    "from modules.data_parsing import parsing_md_sentence\n",
    "\n",
    "folder_path = Path(\"docs_data/test_data/\")\n",
    "\n",
    "file_names = [f.stem for f in folder_path.glob(\"*.jsonl\")]\n",
    "\n",
    "print(file_names)\n",
    "\n",
    "for FILE_NAME in file_names:\n",
    "    texts = []\n",
    "    with open(f\"docs_data/test_data/{FILE_NAME}.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                obj = json.loads(line)\n",
    "                texts.append(obj[\"text\"])\n",
    "\n",
    "    masking_path = f\"docs_data/masking_data/masking_{FILE_NAME}.json\"\n",
    "\n",
    "    labeled_data = []\n",
    "    for text in texts:\n",
    "        chunks = parsing_md_sentence(text, masking_path)\n",
    "        if len(chunks) > 0:\n",
    "            chunks[-1]['label'] = 1\n",
    "            labeled_data.extend(chunks)\n",
    "        \n",
    "    OUTPUT_PATH = f\"docs_data/embedding_data_2/embedding_{FILE_NAME}.jsonl\"\n",
    "\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(len(labeled_data) - 1):\n",
    "            original_label = int(labeled_data[i][\"label\"])\n",
    "            inverted_label = 1 - original_label\n",
    "\n",
    "            record = {\n",
    "                \"text_a\": labeled_data[i][\"text\"],\n",
    "                \"text_b\": labeled_data[i + 1][\"text\"],\n",
    "                \"label\": inverted_label\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047c40f",
   "metadata": {},
   "source": [
    "## 유사도 검색 학습 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c7af1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_NAME = \"file_001\"\n",
    "\n",
    "sections = []\n",
    "\n",
    "with open(f\"docs_data/test_data/{FILE_NAME}.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            obj = json.loads(line)\n",
    "            sections.append(obj)\n",
    "\n",
    "\n",
    "OUTPUT_PATH = f\"docs_data/embedding_data/vector_similarity_{FILE_NAME}.jsonl\"\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for section in sections:\n",
    "        record = {\n",
    "            \"text_a\": section[\"text\"],\n",
    "            \"text_b\": section[\"similar_text\"],\n",
    "            \"label\": 1\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "        record = {\n",
    "            \"text_a\": section[\"text\"],\n",
    "            \"text_b\": section[\"warning_text\"],\n",
    "            \"label\": 0\n",
    "        }\n",
    "        \n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23993f6e",
   "metadata": {},
   "source": [
    "## 색인 학습 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc105472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data_categorize import update_category_from_prompt\n",
    "\n",
    "def create_index(section, category_list) :\n",
    "    category = \"\"\n",
    "    for i, depth_category in enumerate(category_list):\n",
    "        category += f\"\\tdepth_{i} : \"\n",
    "        if len(depth_category) < 1 :\n",
    "            category += \"카테고리 생성 필요\"\n",
    "            continue\n",
    "        for s in depth_category:\n",
    "            category += (s+\", \")\n",
    "        category += \"\\n\"\n",
    "\n",
    "    section_summary = section[\"summary\"]\n",
    "    section_text = section[\"text\"]\n",
    "\n",
    "    indexing_system_prompt = \"\"\" \n",
    "        너는 기술 문서 색인 전문가야.\n",
    "\n",
    "        아래 문서의 맥락을 참고해서\n",
    "        \"현재 섹션\" 하나에 대한 색인 하나만 생성해.\n",
    "\n",
    "        규칙:\n",
    "        - 점(.)으로 구분된 계층 구조로 반드시 한 줄만 출력\n",
    "        - 카테고리는 현재 존재하는 카테고리를 최우선으로 찾고, 존재하지 않을 경우 새 카테고리를 생성\n",
    "        - **비슷한 이름의 카테고리는 반드시 하나로 통합**\n",
    "        - 각 depth별 카테고리는 반드시 하나를 선택하거나 생성되어야 하며, 6단계 이상 들어가면 안됨\n",
    "        - 문맥이 조금 달라도 같은 의미면 항상 같은 색인\n",
    "        - 색인만 출력 (설명 금지)\n",
    "\n",
    "        색인 예시:\n",
    "        문서등록로직.문서등록로직.태그(요약)-내용(정리)-형식(형식)계층기반.섹션분할및태그(요약)매핑\n",
    "    \"\"\"\n",
    "\n",
    "    indexing_user_prompt = f\"\"\"\n",
    "        섹션 요약 :\n",
    "            {section_summary}\n",
    "\n",
    "        섹션 내용 :\n",
    "            {section_text}\n",
    "\n",
    "        현재 존재하는 카테고리 :\n",
    "            {category}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-5.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": indexing_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": indexing_user_prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    index, updated_category = update_category_from_prompt(response.choices[0].message.content, category_list)\n",
    "\n",
    "    return index, updated_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee81367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = []\n",
    "\n",
    "with open(f\"docs_data/test_data/{FILE_NAME}.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            obj = json.loads(line)\n",
    "            sections.append(obj)\n",
    "\n",
    "\n",
    "OUTPUT_PATH = f\"docs_data/indexing_data/index_data_{FILE_NAME}.jsonl\"\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for section in sections:\n",
    "\n",
    "        index, category_list = create_index(section, category_list)\n",
    "\n",
    "        record = {\n",
    "            \"text\" : section[\"text\"],\n",
    "            \"index\" : index\n",
    "        }\n",
    "        \n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73575d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['문서통합'],\n",
       " ['서버매뉴얼'],\n",
       " ['보안정책반영', 'RDB데이터구조설계'],\n",
       " ['접속가이드', '형식및구성의도분석', '데이터전처리과정', '시나리오설명', '논의메모', '섹션분할및태그매핑'],\n",
       " ['시나리오설명',\n",
       "  '데이터전처리과정',\n",
       "  '논의메모',\n",
       "  'AI기반3Way병합초안생성(ServerZone)',\n",
       "  '버전관리및충돌시나리오',\n",
       "  '섹션분할및태그매핑'],\n",
       " ['로컬작성및실시간보안마스킹(IndexedDB토큰매핑)',\n",
       "  '익명화데이터전송및서버유사도분석',\n",
       "  'AI기반3Way병합초안생성(ServerZone)',\n",
       "  'AI기반3Way병합검토(LocalZone)',\n",
       "  '보안정보토큰게시및권한기반실제값조회',\n",
       "  '데이터전처리과정',\n",
       "  '태그기반비관련콘텐츠필터링',\n",
       "  '프로토타입범위및중간발표계획',\n",
       "  '추후개발방향및핵심기능특화계획',\n",
       "  '논의메모',\n",
       "  '섹션분할및태그매핑',\n",
       "  '내용형식유사도비교및LLM통합제안']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skn19_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
